{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proprietary-emphasis",
   "metadata": {},
   "source": [
    "Definitions:\n",
    "* The Q-function tells us the sum of total (discounted) rewards if we start in state s, take action a, and then act optimally from there\n",
    "* The value function tells us the sum of total (discounted) rewards if we start in state s and act optimally from there\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s, a) &= \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big] \\\\\n",
    "V(s) \n",
    "    &= \\max_a Q(s, a) \\\\\n",
    "    &= \\max_a \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined as part of the setup\n",
    "states = [...]  # all states\n",
    "actions = [...]  # all actions\n",
    "gamma = 0.9  # Discount factor\n",
    "T = 1000000  # max time\n",
    "\n",
    "def T(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Probability of ending in state s_new when starting in state s and\n",
    "    taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def R(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Reward for going from state s to state s_new by taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "## SLOW VERSION\n",
    "def V(s, t):\n",
    "    \"\"\"\n",
    "    Value function for a state: expected reward for starting in\n",
    "    s at time t and then acting optimally\n",
    "    \"\"\"\n",
    "    if t < T:\n",
    "        # Take max over actions\n",
    "        best_so_far = -np.inf\n",
    "        for a in actions:\n",
    "            # Compute the expectation over possible next states\n",
    "            q = 0\n",
    "            for s_new in states:\n",
    "                transition_prob = T(s, a, s_new)\n",
    "                total_reward = R(s, a, s_new) + gamma * V(s_new, t+1)\n",
    "                q + = transition_prob * total_reward\n",
    "\n",
    "            value = max(q, best_so_far)\n",
    "            return value\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "## FAST VERSION\n",
    "V_arr = np.zeros([len(states), T])\n",
    "for t in range(T-1, -1, -1):\n",
    "    for s in states:\n",
    "        # Take max over actions\n",
    "        best_so_far = -np.inf\n",
    "        for a in actions:\n",
    "            # Compute the expectation over possible next states\n",
    "            q = 0  # Q(s, a)\n",
    "            for s_new in states:\n",
    "                transition_prob = T(s, a, s_new)\n",
    "                total_reward = R(s, a, s_new) + gamma * V_arr[s_new, t+1]\n",
    "                q += transition_prob * total_reward\n",
    "\n",
    "            best_so_far = max(best_so_far, q)\n",
    "\n",
    "        V_arr[s, t] = best_so_far\n",
    "\n",
    "def V_fast(state, time):\n",
    "    return V_arr[state, time]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b5412",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s, a) &= \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big] \\\\\n",
    "V(s) \n",
    "    &= \\max_a Q(s, a) \\\\\n",
    "    &= \\max_a \\sum_{s'} T(s, a, s') \\big[ R(s, a, s') + \\gamma V(s') \\big]\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
